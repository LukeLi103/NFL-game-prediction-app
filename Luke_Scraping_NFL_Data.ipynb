{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d3RH_mti6zR",
        "outputId": "a8991a31-8040-4406-b198-5a9df02ed1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.14.1)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install bs4 requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dnpHZcel8UK"
      },
      "source": [
        "### Scraping NFL Weather Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imFBSCuYl9sU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pprint import pprint\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LHn-dssmMji"
      },
      "outputs": [],
      "source": [
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "emuhjBivl_-i",
        "outputId": "879e1701-0ca2-48bf-f5ca-d345a047d872"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-535437191.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mrowData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "rowData = []\n",
        "\n",
        "for season in [2021, 2022, 2023, 2024]:\n",
        "    for i in range(1, 19):\n",
        "\n",
        "        page = requests.get(f\"https://www.nflweather.com/week/{season}/week-{i}\")\n",
        "\n",
        "        # scrape webpage\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "        divs = soup.find_all('div', class_='game-box w-100 d-flex flex-column flex-lg-row align-items-center shadow-box rounded my-2 py-1')\n",
        "\n",
        "        for div in divs:\n",
        "            data = [season, i]\n",
        "            date = div.find('div', class_='fw-bold text-wrap').getText()\n",
        "            data.append(date.strip())\n",
        "\n",
        "            awayTeam = div.find('div', class_='flex-centered flex-column me-1 ms-xxl-auto')\n",
        "            data.append(awayTeam.getText().strip())\n",
        "\n",
        "            homeTeam = div.find('div', class_='flex-centered flex-column me-xxl-auto')\n",
        "            data.append(homeTeam.getText().strip())\n",
        "\n",
        "            awayPoints = div.find('div', class_='game-points ps-1 pe-2')\n",
        "            data.append(int(awayPoints.getText().strip()))\n",
        "\n",
        "            homePoints = div.find('div', class_='game-points pe-1 ps-2')\n",
        "            data.append(int(homePoints.getText().strip()))\n",
        "\n",
        "            try:\n",
        "                weather = div.find_all('div', class_='mx-2')\n",
        "                for w in weather:\n",
        "                    data.append(w.find('span').getText())\n",
        "\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            wind = div.find('div', class_='text-break col-md-2 mb-1 px-1 flex-centered')\n",
        "            wind_attributes = wind.getText().strip().split()\n",
        "            wind_speed = wind_attributes[1]\n",
        "            data.append(wind_speed)\n",
        "\n",
        "            rowData.append(data)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "df = pd.DataFrame(rowData, columns = ['Season', 'Week_Number', 'Date', 'Away_Team', 'Home_Team', 'Away_Points', 'Home_Points', 'Temperature', 'Weather', 'Wind Speed'])\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xFqw_41mUSf"
      },
      "outputs": [],
      "source": [
        "df.to_csv('weather_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Neugat-l-th"
      },
      "source": [
        "### Scraping NFL Boxscore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybdNCwf_5AGT",
        "outputId": "9b6ba8bd-d5b9-4141-9b06-27818db984e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 285 boxscores\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-36-4222437521.py:14: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  link_tag = tr.find(\"a\", text=\"boxscore\")\n"
          ]
        }
      ],
      "source": [
        "# scrape every single boxscore link from the 2024 NFL season\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "url = \"https://www.pro-football-reference.com/years/2024/games.htm\"\n",
        "resp = requests.get(url)\n",
        "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "table = soup.find(\"table\", {\"id\": \"games\"})\n",
        "rows = table.tbody.find_all(\"tr\")\n",
        "\n",
        "links = []\n",
        "for tr in rows:\n",
        "  link_tag = tr.find(\"a\", text=\"boxscore\")\n",
        "  if link_tag and link_tag['href'].startswith(\"/boxscores/\"):\n",
        "    links.append(f\"https://www.pro-football-reference.com{link_tag['href']}\")\n",
        "\n",
        "print(f\"Found {len(links)} boxscores\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OOlLxMGenck0"
      },
      "outputs": [],
      "source": [
        "# TODO: scrape each game from pro-football reference\n",
        "# https://www.pro-football-reference.com/boxscores/202502090phi.htm#all_team_stats\n",
        "\n",
        "# Reminder: scrape homeTeam, awayTeam, homeScore, awayScore\n",
        "# then scrape all of the variables in \"Team Stats\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4 import Comment\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def parse_data_from_table(table):\n",
        "  away_team, home_team = table.columns[1:]\n",
        "  table.columns = ['Category', home_team, away_team]\n",
        "  # table.columns[0] = 'Category'\n",
        "  stats_by_team = {}\n",
        "  # print(away_team, home_team)\n",
        "  for idx, row in table.iterrows():\n",
        "    if row['Category'] == 'Rush-Yds-TDs':\n",
        "      rushes, rush_yds, rush_tds = row[away_team].split('-')\n",
        "      stats_by_team[f\"Away Team Rushes\"] = int(rushes)\n",
        "      stats_by_team[f\"Away Team Rush Yards\"] = int(rush_yds)\n",
        "      stats_by_team[f\"Away Team Rush TDs\"] = int(rush_tds)\n",
        "      rushes, rush_yds, rush_tds = row[home_team].split('-')\n",
        "      stats_by_team[f\"Home Team Rushes\"] = int(rushes)\n",
        "      stats_by_team[f\"Home Team Rush Yards\"] = int(rush_yds)\n",
        "      stats_by_team[f\"Home Team Rush TDs\"] = int(rush_tds)\n",
        "    elif row['Category'] == 'Cmp-Att-Yd-TD-INT':\n",
        "      completions, pass_attempts, pass_yds, pass_tds, ints = row[away_team].split('-')\n",
        "      stats_by_team[f\"Away Team Completions\"] = int(completions)\n",
        "      stats_by_team[f\"Away Team Pass Attempts\"] = int(pass_attempts)\n",
        "      stats_by_team[f\"Away Team Pass Yards\"] = int(pass_yds)\n",
        "      stats_by_team[f\"Away Team Pass TDs\"] = int(pass_tds)\n",
        "      stats_by_team[f\"Away Team Interceptions\"] = int(ints)\n",
        "      completions, pass_attempts, pass_yds, pass_tds, ints = row[home_team].split('-')\n",
        "      stats_by_team[f\"Home Team Completions\"] = int(completions)\n",
        "      stats_by_team[f\"Home Team Pass Attempts\"] = int(pass_attempts)\n",
        "      stats_by_team[f\"Home Team Pass Yards\"] = int(pass_yds)\n",
        "      stats_by_team[f\"Home Team Pass TDs\"] = int(pass_tds)\n",
        "      stats_by_team[f\"Home Team Interceptions\"] = int(ints)\n",
        "    elif row['Category'] == 'Sacked-Yards':\n",
        "      sacks, sack_yds = row[away_team].split('-')\n",
        "      stats_by_team[f\"Away Team Sacks\"] = int(sacks)\n",
        "      stats_by_team[f\"Away Team Sack Yards\"] = int(sack_yds)\n",
        "      sacks, sack_yds = row[home_team].split('-')\n",
        "      stats_by_team[f\"Home Team Sacks\"] = int(sacks)\n",
        "      stats_by_team[f\"Home Team Sack Yards\"] = int(sack_yds)\n",
        "    elif row['Category'] == 'Fumbles-Lost':\n",
        "      fumbles, fumbles_lost = row[away_team].split('-')\n",
        "      stats_by_team[f\"Away Team Fumbles\"] = int(fumbles)\n",
        "      stats_by_team[f\"Away Team Fumbles Lost\"] = int(fumbles_lost)\n",
        "      fumbles, fumbles_lost = row[home_team].split('-')\n",
        "      stats_by_team[f\"Home Team Fumbles\"] = int(fumbles)\n",
        "      stats_by_team[f\"Home Team Fumbles Lost\"]  = int(fumbles_lost)\n",
        "    elif row['Category'] == 'Penalties-Yards':\n",
        "      penalties, penalty_yds = row[away_team].split('-')\n",
        "      stats_by_team[f\"Away Team Penalties\"] = int(penalties)\n",
        "      stats_by_team[f\"Away Team Penalty Yards\"] = int(penalty_yds)\n",
        "      penalties, penalty_yds = row[home_team].split('-')\n",
        "      stats_by_team[f\"Home Team Penalties\"] = int(penalties)\n",
        "      stats_by_team[f\"Home Team Penalty Yards\"] = int(penalty_yds)\n",
        "    elif row['Category'] == 'Third Down Conv.':\n",
        "      converted, attempts = row[away_team].split('-')\n",
        "      stats_by_team[f\"Away Team 3rd Down Conversions\"] = int(converted)\n",
        "      stats_by_team[f\"Away Team 3rd Down Attempts\"] = int(attempts)\n",
        "      converted, attempts = row[home_team].split('-')\n",
        "      stats_by_team[f\"Home Team 3rd Down Conversions\"] = int(converted)\n",
        "      stats_by_team[f\"Home Team 3rd Down Attempts\"] = int(attempts)\n",
        "    elif row['Category'] == 'Fourth Down Conv.':\n",
        "      converted, attempts = row[away_team].split('-')\n",
        "      stats_by_team[f\"Away Team 4th Down Conversions\"] = int(converted)\n",
        "      stats_by_team[f\"Away Team 4th Down Attempts\"] = int(attempts)\n",
        "      converted, attempts = row[home_team].split('-')\n",
        "      stats_by_team[f\"Home Team 4th Down Conversions\"] = int(converted)\n",
        "      stats_by_team[f\"Home Team 4th Down Attempts\"] = int(attempts)\n",
        "    else:\n",
        "      stats_by_team[f\"Away Team {row['Category']}\"] = row[away_team]\n",
        "      stats_by_team[f\"Home Team {row['Category']}\"] = row[home_team]\n",
        "\n",
        "  stats_by_team_df = pd.DataFrame.from_dict(stats_by_team, orient='index')\n",
        "  return stats_by_team_df\n",
        "\n",
        "\n",
        "def scrape_team_stats_table(url):\n",
        "  resp = requests.get(url)\n",
        "  soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "  # Find the \"Team Stats\" table by its surrounding comment/wrapper or table captions\n",
        "  div = soup.find(id=\"all_team_stats\")\n",
        "  if div:\n",
        "      # Pro‑Football‑Reference sometimes wraps tables in <!-- --> comments\n",
        "      commented_html = div.find(string=lambda text: isinstance(text, Comment))\n",
        "      inner_soup = BeautifulSoup(commented_html, \"html.parser\")\n",
        "      table = inner_soup.find(\"table\")\n",
        "  else:\n",
        "      table = soup.find(\"table\", {\"id\": \"team_stats\"})\n",
        "\n",
        "  df = pd.read_html(str(table))[0]\n",
        "  return df\n",
        "\n",
        "full_data = pd.DataFrame()\n",
        "for link in links:\n",
        "  df = scrape_team_stats_table(link)\n",
        "  stats_df = parse_data_from_table(df).transpose()\n",
        "  stats_df.index = [url.split('/')[-1].split('.')[0]]\n",
        "  full_data = pd.concat([full_data, stats_df], axis=0)\n",
        "  time.sleep(3.1)\n",
        "\n",
        "# NOTE: [command/control] + D to multi-select\n",
        "\n",
        "# off_first_downs\n",
        "# off_rushes\n",
        "# off_rushing_yds\n",
        "# off_rushing_tds\n",
        "# off_completions\n",
        "# off_pass_attempts\n",
        "# off_pass_yds\n",
        "# off_pass_tds\n",
        "# off_ints\n",
        "# off_sacks\n",
        "# off_sack_yds\n",
        "# off_net_pass_yds\n",
        "# off_total_yds\n",
        "# off_fumbles\n",
        "# off_fumbles_lost\n",
        "# off_turnovers\n",
        "# off_penalties\n",
        "# off_penalty_yds\n",
        "# off_third_down_conversions\n",
        "# off_fourth_down_conversions\n",
        "# off_possession_time\n",
        "# def_first_downs\n",
        "# def_rushes\n",
        "# def_rushing_yds\n",
        "# def_rushing_tds\n",
        "# def_completions\n",
        "# def_pass_attempts\n",
        "# def_pass_yds\n",
        "# def_pass_tds\n",
        "# def_ints\n",
        "# def_sacks\n",
        "# def_sack_yds\n",
        "# def_net_pass_yds\n",
        "# def_total_yds\n",
        "# def_fumbles\n",
        "# def_fumbles_lost\n",
        "# def_turnovers\n",
        "# def_penalties\n",
        "# def_penalty_yds\n",
        "# def_third_down_conversions\n",
        "# def_fourth_down_conversions\n",
        "# def_possession_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZrOe8v8wpH8p"
      },
      "outputs": [],
      "source": [
        "full_data.to_csv('boxscore_data.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rERBhD4BVLqv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}